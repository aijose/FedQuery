{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedQuery Demo: FOMC Agentic RAG Research Assistant\n",
    "\n",
    "This notebook demonstrates the FedQuery system end-to-end:\n",
    "\n",
    "1. **Raw retrieval** — bi-encoder similarity search over FOMC document chunks (no LLM)\n",
    "2. **Reranker-enhanced retrieval** — cross-encoder reranking for improved relevance\n",
    "3. **Full LangGraph agent** — assess → search → evaluate confidence → synthesize → validate → respond\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run `fedquery ingest --year 2024` first to populate the ChromaDB store\n",
    "- Sections 1-2 work without an API key\n",
    "- Section 3 requires `ANTHROPIC_API_KEY` in `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is on sys.path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from config.settings import get_settings\n",
    "from src.vectorstore.chroma_store import ChromaStore\n",
    "from src.embedding.sentence_transformer import SentenceTransformerEmbeddingProvider\n",
    "\n",
    "settings = get_settings()\n",
    "store = ChromaStore(path=str(settings.chroma_path))\n",
    "embedding_provider = SentenceTransformerEmbeddingProvider(settings.fedquery_embedding_model)\n",
    "\n",
    "print(f\"Embedding model: {settings.fedquery_embedding_model} ({embedding_provider.dimension}d)\")\n",
    "print(f\"ChromaDB path:   {settings.chroma_path}\")\n",
    "print(f\"Chunks in store: {store.count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Retrieval Only (no LLM needed)\n",
    "\n",
    "### Raw Bi-Encoder Retrieval\n",
    "\n",
    "The first stage uses all-MiniLM-L6-v2 to embed the query and find the closest chunks\n",
    "by cosine similarity in ChromaDB. This is fast but can miss nuanced relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent.mcp_client import create_search_fn\n",
    "\n",
    "search_fn = create_search_fn(store, embedding_provider)\n",
    "\n",
    "queries = [\n",
    "    \"What was the federal funds rate target range in January 2024?\",\n",
    "    \"How did the FOMC characterize inflation risks in 2024?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {q}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    results = search_fn(q, top_k=5)\n",
    "    print(f\"{'Doc':<45} {'Date':<12} {'Section':<25} {'Score':>6}\")\n",
    "    print(\"-\" * 92)\n",
    "    for r in results:\n",
    "        doc = r['document_name'][:44]\n",
    "        date = r['document_date'][:11]\n",
    "        sec = r['section_header'][:24]\n",
    "        print(f\"{doc:<45} {date:<12} {sec:<25} {r['relevance_score']:>6.3f}\")\n",
    "        print(f\"  {r['chunk_text'][:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrieval.reranker import CrossEncoderReranker\n",
    "\n",
    "reranker = CrossEncoderReranker(settings.fedquery_reranker_model)\n",
    "reranked_search_fn = create_search_fn(store, embedding_provider, reranker=reranker)\n",
    "\n",
    "print(f\"Reranker model: {reranker.model_name}\")\n",
    "print(f\"Strategy: over-fetch 3x from bi-encoder, rerank with cross-encoder\\n\")\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {q}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    results = reranked_search_fn(q, top_k=5)\n",
    "    print(f\"{'Doc':<45} {'Date':<12} {'Section':<25} {'Score':>6}\")\n",
    "    print(\"-\" * 92)\n",
    "    for r in results:\n",
    "        doc = r['document_name'][:44]\n",
    "        date = r['document_date'][:11]\n",
    "        sec = r['section_header'][:24]\n",
    "        print(f\"{doc:<45} {date:<12} {sec:<25} {r['relevance_score']:>6.3f}\")\n",
    "        print(f\"  {r['chunk_text'][:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Full LangGraph Agent (requires ANTHROPIC_API_KEY)\n\nThe agent workflow:\n\n1. **assess_query** — LLM classifies whether retrieval is needed\n2. **search_corpus** — retrieval via MCP server subprocess (stdio protocol)\n3. **evaluate_confidence** — score thresholds (high >= 0.55, medium >= 0.40, low >= 0.25)\n4. **reformulate_query** — if low confidence, LLM rephrases (up to 2 retries)\n5. **synthesize_answer** — LLM generates answer grounded in retrieved chunks with [Source N] citations\n6. **validate_citations** — verify each citation maps to an actual chunk\n7. **respond** — format final answer with sources, or return uncertainty message\n\nRetrieval goes through the MCP server: the agent spawns it as a subprocess and\ncommunicates via stdio, matching how Claude Desktop and other MCP hosts work.\nSet `FEDQUERY_USE_MCP=false` to fall back to direct in-process calls."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.agent.mcp_client import MCPSearchClient, create_mcp_search_fn\nfrom src.agent.graph import build_graph\n\n# Connect to MCP server subprocess (reuse across all queries in this section)\nmcp_client = MCPSearchClient()\nmcp_client.connect()\n\nsearch_fn = create_mcp_search_fn(mcp_client)\ngraph = build_graph(search_fn)\n\n\ndef ask(question: str):\n    \"\"\"Run the agent graph and display the result.\"\"\"\n    print(f\"Question: {question}\\n\")\n    result = graph.invoke({\n        \"query\": question,\n        \"retrieved_chunks\": [],\n        \"confidence\": \"insufficient\",\n        \"reformulation_attempts\": 0,\n        \"reformulated_query\": None,\n        \"answer\": None,\n        \"citations\": [],\n        \"needs_retrieval\": False,\n    })\n    print(f\"Confidence: {result['confidence']}\")\n    print(f\"Reformulations: {result['reformulation_attempts']}\")\n    print(f\"Chunks retrieved: {len(result['retrieved_chunks'])}\")\n    print(f\"Citations: {len(result['citations'])}\")\n    print(f\"\\n{'─'*80}\")\n    print(result[\"answer\"])\n\n\nprint(\"Agent graph built. Nodes:\", list(graph.get_graph().nodes))\nprint(\"Retrieval mode: MCP (stdio subprocess)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factual question\n",
    "ask(\"What was the federal funds rate target range set by the FOMC in January 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-document question\n",
    "ask(\"How did the FOMC's characterization of inflation change between January and September 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section-specific question\n",
    "ask(\"What did FOMC participants discuss about quantitative tightening in 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-scope question — should return uncertainty response\n",
    "ask(\"What is the European Central Bank's current interest rate?\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Clean up MCP server subprocess\nmcp_client.close()\nprint(\"MCP server subprocess stopped.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated three levels of the FedQuery pipeline:\n",
    "\n",
    "| Stage | LLM Required | What it does |\n",
    "|-------|-------------|-------------|\n",
    "| Bi-encoder retrieval | No | Fast cosine similarity search over FOMC chunks |\n",
    "| + Cross-encoder reranker | No | Reranks candidates for better precision |\n",
    "| Full LangGraph agent | Yes | Assesses, retrieves, synthesizes, cites, validates |\n",
    "\n",
    "Key behaviors observed:\n",
    "- Factual questions get high-confidence, well-cited answers\n",
    "- Cross-document questions pull from multiple meeting dates\n",
    "- Out-of-scope questions correctly return uncertainty responses\n",
    "- All citations are validated against actual retrieved chunks\n",
    "\n",
    "For retrieval benchmark data (HNSW vs IVF, chunking grid, reranker impact), see\n",
    "[`optimization_efforts.md`](optimization_efforts.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}