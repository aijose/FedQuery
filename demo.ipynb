{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FedQuery Demo: FOMC Agentic RAG Research Assistant\n\nThis notebook demonstrates the FedQuery system end-to-end:\n\n1. **Raw retrieval** — bi-encoder similarity search over FOMC document chunks (no LLM)\n2. **Reranker-enhanced retrieval** — cross-encoder reranking for improved relevance\n3. **Full LangGraph agent** — assess (with date extraction + adaptive top_k) → two-pass search → evaluate confidence → synthesize → validate → respond\n\n**Prerequisites:**\n- Run `fedquery ingest --years 2024` first to populate the ChromaDB store (or `--years 2021 2022 2023 2024 2025` for the full corpus)\n- Sections 1-2 work without an API key\n- Section 3 requires `ANTHROPIC_API_KEY` in `.env`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is on sys.path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from config.settings import get_settings\n",
    "from src.vectorstore.chroma_store import ChromaStore\n",
    "from src.embedding.sentence_transformer import SentenceTransformerEmbeddingProvider\n",
    "\n",
    "settings = get_settings()\n",
    "store = ChromaStore(path=str(settings.chroma_path))\n",
    "embedding_provider = SentenceTransformerEmbeddingProvider(settings.fedquery_embedding_model)\n",
    "\n",
    "print(f\"Embedding model: {settings.fedquery_embedding_model} ({embedding_provider.dimension}d)\")\n",
    "print(f\"ChromaDB path:   {settings.chroma_path}\")\n",
    "print(f\"Chunks in store: {store.count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2: Retrieval Only (no LLM needed)\n\n### Raw Bi-Encoder Retrieval\n\nThe first stage uses BAAI/bge-small-en-v1.5 to embed the query and find the closest chunks\nby cosine similarity in ChromaDB. This is fast but can miss nuanced relevance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.agent.mcp_client import create_search_fn\n\nsearch_fn = create_search_fn(store, embedding_provider)\n\nqueries = [\n    \"What was the federal funds rate target range in January 2024?\",\n    \"How did the FOMC characterize inflation risks in 2024?\",\n]\n\nfor q in queries:\n    print(f\"\\n{'='*80}\")\n    print(f\"Query: {q}\")\n    print(f\"{'='*80}\")\n    results = search_fn(q, top_k=10)\n    print(f\"{'Doc':<45} {'Date':<12} {'Section':<25} {'Score':>6}\")\n    print(\"-\" * 92)\n    for r in results:\n        doc = r['document_name'][:44]\n        date = r['document_date'][:11]\n        sec = r['section_header'][:24]\n        print(f\"{doc:<45} {date:<12} {sec:<25} {r['relevance_score']:>6.3f}\")\n        print(f\"  {r['chunk_text'][:120]}...\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.retrieval.reranker import CrossEncoderReranker\n\nreranker = CrossEncoderReranker(settings.fedquery_reranker_model)\nreranked_search_fn = create_search_fn(store, embedding_provider, reranker=reranker)\n\nprint(f\"Reranker model: {reranker.model_name}\")\nprint(f\"Strategy: over-fetch 3x from bi-encoder, rerank with cross-encoder\\n\")\n\nfor q in queries:\n    print(f\"\\n{'='*80}\")\n    print(f\"Query: {q}\")\n    print(f\"{'='*80}\")\n    results = reranked_search_fn(q, top_k=10)\n    print(f\"{'Doc':<45} {'Date':<12} {'Section':<25} {'Score':>6}\")\n    print(\"-\" * 92)\n    for r in results:\n        doc = r['document_name'][:44]\n        date = r['document_date'][:11]\n        sec = r['section_header'][:24]\n        print(f\"{doc:<45} {date:<12} {sec:<25} {r['relevance_score']:>6.3f}\")\n        print(f\"  {r['chunk_text'][:120]}...\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Full LangGraph Agent (requires ANTHROPIC_API_KEY)\n\nThe agent workflow:\n\n1. **assess_query** — LLM classifies whether retrieval is needed, extracts date ranges for temporal filtering, and estimates `top_k_hint` (how many results to fetch based on query scope)\n2. **search_corpus** — two-pass retrieval when date hints are present: (1) filtered pass with date range, (2) unfiltered pass, merged and deduped. Uses adaptive `top_k` from the hint (default 10, up to 50 for multi-year queries).\n3. **evaluate_confidence** — score thresholds (high >= 0.55, medium >= 0.40, low >= 0.25)\n4. **reformulate_query** — if low confidence, LLM rephrases (up to 2 retries)\n5. **synthesize_answer** — LLM generates answer grounded in retrieved chunks with [Source N] citations\n6. **validate_citations** — verify each citation maps to an actual chunk\n7. **respond** — format final answer with sources, or return uncertainty message\n\nThis cell uses the same retrieval backend as the CLI (`fedquery ask`), controlled by `FEDQUERY_USE_MCP`:\n- **`true` (default)**: Spawns MCP server as subprocess, communicates via stdio (same as Claude Desktop)\n- **`false`**: Direct in-process ChromaStore calls (faster, no subprocess overhead)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.agent.graph import build_graph\n\n# Use the same retrieval backend as the CLI (controlled by FEDQUERY_USE_MCP setting)\ncleanup_fn = None\n\nif settings.fedquery_use_mcp:\n    from src.agent.mcp_client import MCPSearchClient, create_mcp_search_fn\n    mcp_client = MCPSearchClient()\n    mcp_client.connect()\n    search_fn = create_mcp_search_fn(mcp_client)\n    cleanup_fn = mcp_client.close\n    retrieval_mode = \"MCP (stdio subprocess)\"\nelse:\n    from src.agent.mcp_client import create_direct_search_fn\n    search_fn = create_direct_search_fn(store, embedding_provider)\n    retrieval_mode = \"Direct (in-process ChromaStore)\"\n\ngraph = build_graph(search_fn)\n\n\ndef ask(question: str):\n    \"\"\"Run the agent graph and display the result.\"\"\"\n    print(f\"Question: {question}\\n\")\n    result = graph.invoke({\n        \"query\": question,\n        \"retrieved_chunks\": [],\n        \"confidence\": \"insufficient\",\n        \"reformulation_attempts\": 0,\n        \"reformulated_query\": None,\n        \"answer\": None,\n        \"citations\": [],\n        \"needs_retrieval\": True,\n        \"metadata_hints\": None,\n        \"top_k_hint\": None,\n    })\n    print(f\"Confidence: {result['confidence']}\")\n    print(f\"Reformulations: {result['reformulation_attempts']}\")\n    print(f\"Chunks retrieved: {len(result['retrieved_chunks'])}\")\n    print(f\"Citations: {len(result['citations'])}\")\n    hints = result.get('metadata_hints')\n    if hints:\n        print(f\"Date filter: {hints.get('date_start')} to {hints.get('date_end')}\")\n    top_k = result.get('top_k_hint')\n    if top_k:\n        print(f\"Adaptive top_k: {top_k}\")\n    print(f\"\\n{'─'*80}\")\n    print(result[\"answer\"])\n\n\nprint(f\"Agent graph built. Nodes: {list(graph.get_graph().nodes)}\")\nprint(f\"Retrieval mode: {retrieval_mode}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factual question\n",
    "ask(\"What was the federal funds rate target range set by the FOMC in January 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-document question\n",
    "ask(\"How did the FOMC's characterization of inflation change between January and September 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section-specific question\n",
    "ask(\"What did FOMC participants discuss about quantitative tightening in 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-scope question — should return uncertainty response\n",
    "ask(\"What is the European Central Bank's current interest rate?\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Clean up MCP server subprocess (if used)\nif cleanup_fn:\n    cleanup_fn()\n    print(\"MCP server subprocess stopped.\")\nelse:\n    print(\"Direct mode — no cleanup needed.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated three levels of the FedQuery pipeline:\n",
    "\n",
    "| Stage | LLM Required | What it does |\n",
    "|-------|-------------|-------------|\n",
    "| Bi-encoder retrieval | No | Fast cosine similarity search over FOMC chunks |\n",
    "| + Cross-encoder reranker | No | Reranks candidates for better precision |\n",
    "| Full LangGraph agent | Yes | Assesses, retrieves, synthesizes, cites, validates |\n",
    "\n",
    "Key behaviors observed:\n",
    "- Factual questions get high-confidence, well-cited answers\n",
    "- Cross-document questions pull from multiple meeting dates\n",
    "- Out-of-scope questions correctly return uncertainty responses\n",
    "- All citations are validated against actual retrieved chunks\n",
    "\n",
    "For retrieval benchmark data (HNSW vs IVF, chunking grid, reranker impact), see\n",
    "[`optimization_efforts.md`](optimization_efforts.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}